{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance (Solution)\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for generating a robust test suite. It covers generating simple tests, brainstorming edge cases, and creating the necessary fixtures for professional-grade database testing.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the application's source code to provide the LLM with the necessary context to write accurate tests. A good prompt for test generation should always include the code that needs to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:32:26,460 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:32:26,466 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o-mini latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:32:26,471 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o-mini latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy-path model: gpt-4o (provider: openai)\n",
      "Edge-case model: gpt-4o-mini (provider: openai)\n",
      "Fixture model: gpt-4o-mini (provider: openai)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for each lab task so we can experiment with different models per task.\n",
    "# You can change the model_name for each client to a model available in your environment or the RECOMMENDED_MODELS table.\n",
    "# Use a stronger model for generation of structured tests, and lighter models for edge cases/fixtures to save quota in examples.\n",
    "happy_client, happy_model, happy_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "edge_client, edge_model, edge_provider = setup_llm_client(model_name=\"gpt-4o-mini\")\n",
    "fixture_client, fixture_model, fixture_provider = setup_llm_client(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "print(f\"Happy-path model: {happy_model} (provider: {happy_provider})\")\n",
    "print(f\"Edge-case model: {edge_model} (provider: {edge_provider})\")\n",
    "print(f\"Fixture model: {fixture_model} (provider: {fixture_provider})\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks for the most straightforward type of test: one that verifies the application works as expected when given valid input. We specifically ask for tests for the `POST` and `GET` endpoints. The prompt includes the full application code as context, which is crucial for the LLM to understand the API's structure, expected payloads, and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "from fastapi.testclient import TestClient\n",
      "from main import app  # Assuming the FastAPI app is defined in a file named main.py\n",
      "import pytest\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "@pytest.fixture\n",
      "def user_data():\n",
      "    return {\n",
      "        \"email\": \"testuser@example.com\",\n",
      "        \"password\": \"securepassword123\"\n",
      "    }\n",
      "\n",
      "def test_create_user(user_data):\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 200\n",
      "    assert response.json()[\"email\"] == user_data[\"email\"]\n",
      "\n",
      "def test_read_users(user_data):\n",
      "    # Create a user first\n",
      "    client.post(\"/users/\", json=user_data)\n",
      "    \n",
      "    # Retrieve the list of users\n",
      "    response = client.get(\"/users/\")\n",
      "    assert response.status_code == 200\n",
      "    users = response.json()\n",
      "    assert isinstance(users, list)\n",
      "    assert len(users) > 0\n"
     ]
    }
   ],
   "source": [
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are a Senior QA Engineer writing tests for a FastAPI application using pytest.\n",
    "\n",
    "Based on the application code provided below, please generate two 'happy path' test functions in a single Python script:\n",
    "1. A test named `test_create_user` for the `POST /users/` endpoint. It should create a user and assert that the status code is 200 and the response email matches the input.\n",
    "2. A test named `test_read_users` for the `GET /users/` endpoint. It should first create a user and then assert the status code is 200 and that the response is a list containing at least one user.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Your response should be only the raw Python code for the tests, including necessary imports like `TestClient` from `fastapi.testclient`.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    # Enhance the prompt for more consistent and focused output\n",
    "    enhanced_happy_prompt = prompt_enhancer(happy_path_tests_prompt, model_name=happy_model, client=happy_client, api_provider=happy_provider)\n",
    "    generated_happy_path_tests = get_completion(enhanced_happy_prompt, happy_client, happy_model, happy_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Explanation:**\n",
    "Good testing goes beyond the happy path. This prompt asks the LLM to think about what could go wrong. We specifically request tests for two common failure modes: creating a duplicate resource (which should be disallowed) and requesting a resource that doesn't exist. This demonstrates how AI can be used as a creative partner to brainstorm potential failure points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from main import app  # Assuming your FastAPI app is in a file named main.py\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "@pytest.fixture\n",
      "def create_user():\n",
      "    # Helper function to create a user\n",
      "    response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"name\": \"Test User\"})\n",
      "    return response.json()\n",
      "\n",
      "def test_create_user_duplicate_email(create_user):\n",
      "    response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"name\": \"Another User\"})\n",
      "    assert response.status_code == 400\n",
      "    assert response.json() == {\"detail\": \"Email already registered.\"}  # Adjust based on your error response\n",
      "\n",
      "def test_read_user_not_found():\n",
      "    response = client.get(\"/users/999\")  # Assuming 999 does not exist\n",
      "    assert response.status_code == 404\n",
      "    assert response.json() == {\"detail\": \"User not found.\"}  # Adjust based on your error response\n"
     ]
    }
   ],
   "source": [
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer focused on identifying edge cases.\n",
    "\n",
    "Based on the FastAPI application code provided, write two test functions for common error scenarios:\n",
    "1.  A test named `test_create_user_duplicate_email` that attempts to create a user with an email that already exists. It must assert that the API returns a 400 status code.\n",
    "2.  A test named `test_read_user_not_found` that attempts to GET a user with an ID that does not exist (e.g., 999). It must assert that the API returns a 404 status code.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for these two test functions.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    enhanced_edge_prompt = prompt_enhancer(edge_case_tests_prompt, model_name=edge_model, client=edge_client, api_provider=edge_provider)\n",
    "    generated_edge_case_tests = get_completion(enhanced_edge_prompt, edge_client, edge_model, edge_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "    # In a real scenario, you'd append these to your test file.\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Explanation:**\n",
    "This is the most advanced and most important concept in this lab. A `pytest` fixture is a function that runs before each test to set up a specific state or resource. \n",
    "\n",
    "The prompt asks the LLM to generate a fixture that creates an isolated, in-memory database for testing. This is a best practice because it ensures tests are independent and don't interfere with each other or with the real development database. \n",
    "\n",
    "We specifically instruct the LLM to save this fixture in `tests/conftest.py`. `conftest.py` is a special file that pytest automatically discovers. Fixtures defined here are globally available to all test files in the same directory and subdirectories, making it the ideal place to put shared setup code like a database connection. \n",
    "\n",
    "Finally, we ask the LLM to refactor our original happy-path tests to *use* this fixture by simply adding it as a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture for conftest.py ---\n",
      "import pytest\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker, scoped_session\n",
      "from your_application.models import Base  # Import your Base class from your models\n",
      "\n",
      "@pytest.fixture(scope='function')\n",
      "def db_session():\n",
      "    # Create an in-memory SQLite database\n",
      "    engine = create_engine('sqlite:///:memory:')\n",
      "    # Create a new session factory\n",
      "    session_factory = sessionmaker(bind=engine)\n",
      "    # Create a scoped session\n",
      "    session = scoped_session(session_factory)\n",
      "\n",
      "    # Create all tables\n",
      "    Base.metadata.create_all(engine)\n",
      "\n",
      "    # Yield the session to the test\n",
      "    yield session\n",
      "\n",
      "    # Tear down the database and remove the session\n",
      "    session.remove()\n",
      "    Base.metadata.drop_all(engine)\n",
      "\n",
      "--- Generating Refactored Tests for test_main_with_fixture.py ---\n",
      "from fastapi.testclient import TestClient\n",
      "import pytest\n",
      "\n",
      "@pytest.fixture\n",
      "def user_data():\n",
      "    return {\n",
      "        \"email\": \"testuser@example.com\",\n",
      "        \"password\": \"securepassword123\"\n",
      "    }\n",
      "\n",
      "def test_create_user(db_session, client, user_data):\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 201  # Updated to 201 Created\n",
      "    assert response.json()[\"email\"] == user_data[\"email\"]\n",
      "\n",
      "def test_create_duplicate_user(db_session, client, user_data):\n",
      "    # Create the user first\n",
      "    client.post(\"/users/\", json=user_data)\n",
      "    \n",
      "    # Attempt to create the same user again\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 400  # Assuming 400 Bad Request for duplicate\n",
      "    assert \"detail\" in response.json()  # Check for an error message\n",
      "\n",
      "def test_read_users(db_session, client, user_data):\n",
      "    # Create a user first\n",
      "    create_response = client.post(\"/users/\", json=user_data)\n",
      "    assert create_response.status_code == 201  # Ensure user was created successfully\n",
      "\n",
      "    # Retrieve the list of users\n",
      "    response = client.get(\"/users/\")\n",
      "    assert response.status_code == 200\n",
      "    users = response.json()\n",
      "    \n",
      "    assert isinstance(users, list)\n",
      "    assert len(users) > 0\n",
      "\n",
      "    # Check that the created user is in the list\n",
      "    assert any(user[\"email\"] == user_data[\"email\"] for user in users)\n"
     ]
    }
   ],
   "source": [
    "db_fixture_prompt = f\"\"\"\n",
    "You are an expert in Python testing with pytest and FastAPI.\n",
    "\n",
    "I need to create a `pytest` fixture to provide an isolated, in-memory SQLite database session for each test run. This is a critical best practice for testing database-connected applications.\n",
    "\n",
    "Please generate the Python code for a file named `tests/conftest.py` that contains this fixture.\n",
    "\n",
    "The fixture should:\n",
    "1. Be named `db_session`.\n",
    "2. Configure a SQLAlchemy engine for an in-memory SQLite database.\n",
    "3. Create all database tables before the tests run.\n",
    "4. Yield a database session.\n",
    "5. Clean up the database tables after the tests are complete.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for the `conftest.py` file.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture for conftest.py ---\")\n",
    "if app_code:\n",
    "    enhanced_fixture_prompt = prompt_enhancer(db_fixture_prompt, model_name=fixture_model, client=fixture_client, api_provider=fixture_provider)\n",
    "    generated_db_fixture = get_completion(enhanced_fixture_prompt, fixture_client, fixture_model, fixture_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer refactoring a test suite to use a new database fixture.\n",
    "\n",
    "Given the following tests and the knowledge that a fixture named `db_session` and a `TestClient` instance named `client` are now available from `conftest.py`, please rewrite the tests to use them. The tests should no longer create their own client instances.\n",
    "\n",
    "**Original Tests:**\n",
    "```python\n",
    "{generated_happy_path_tests if 'generated_happy_path_tests' in locals() else ''}\n",
    "```\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for the refactored tests.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests for test_main_with_fixture.py ---\")\n",
    "if app_code and 'generated_happy_path_tests' in locals():\n",
    "    enhanced_refactor_prompt = prompt_enhancer(refactor_tests_prompt, model_name=fixture_model, client=fixture_client, api_provider=fixture_provider)\n",
    "    refactored_tests = get_completion(enhanced_refactor_prompt, fixture_client, fixture_model, fixture_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context or original tests are missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
