{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline (Solution)\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts for generating the `requirements.txt`, `Dockerfile`, and GitHub Actions workflow files. It demonstrates how to prompt for specific, structured configuration-as-code artifacts.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, prompt_enhancer, recommended_models_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INITIALIZING SPECIALIZED LLM CLIENTS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-21 21:42:05,626 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-flash latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies analysis: gemini-2.5-flash (google)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:42:05,883 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:42:05,961 ag_aisoftdev.utils INFO LLM Client configured provider=huggingface model=deepseek-ai/DeepSeek-V3.1 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker configuration: gpt-4o (openai)\n",
      "CI/CD workflows: deepseek-ai/DeepSeek-V3.1 (huggingface)\n",
      "✅ All specialized clients initialized successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize different LLM clients for specialized CI/CD tasks\n",
    "print(\"=== INITIALIZING SPECIALIZED LLM CLIENTS ===\")\n",
    "\n",
    "# Dependency analysis client - Fast model for simple parsing tasks\n",
    "deps_client, deps_model_name, deps_api_provider = setup_llm_client(model_name=\"gemini-2.5-flash\")\n",
    "print(f\"Dependencies analysis: {deps_model_name} ({deps_api_provider})\")\n",
    "\n",
    "# Docker configuration client - Balanced model for infrastructure as code\n",
    "docker_client, docker_model_name, docker_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "print(f\"Docker configuration: {docker_model_name} ({docker_api_provider})\")\n",
    "\n",
    "# CI/CD workflow client - Advanced model for complex YAML generation\n",
    "cicd_client, cicd_model_name, cicd_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "print(f\"CI/CD workflows: {cicd_model_name} ({cicd_api_provider})\")\n",
    "\n",
    "print(\"✅ All specialized clients initialized successfully!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded application code (1277 characters)\n",
      "First 200 characters preview:\n",
      "--------------------------------------------------\n",
      "from typing import List\n",
      "from fastapi import Depends, FastAPI, HTTPException\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import BaseModel\n",
      "from app.db_models import SessionLocal, engine, Base, User...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")\n",
    "else:\n",
    "    print(f\"✅ Successfully loaded application code ({len(app_code)} characters)\")\n",
    "    print(\"First 200 characters preview:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(app_code[:200] + \"...\" if len(app_code) > 200 else app_code)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Explanation:**\n",
    "This challenge demonstrates how prompt enhancement can transform a simple dependency analysis task into a sophisticated, production-ready solution. We start with a basic prompt for analyzing Python imports, but enhance it using our `prompt_enhancer` function to apply advanced prompt engineering techniques.\n",
    "\n",
    "The `prompt_enhancer` automatically:\n",
    "- Assigns the appropriate expert persona (\"You are a Python Dependency Analysis Expert\")\n",
    "- Provides structured context and clear task definitions\n",
    "- Sets explicit output format expectations\n",
    "- Applies best practices for reliable, consistent results\n",
    "\n",
    "We use a fast model (Gemini Flash) for this straightforward parsing task, demonstrating how different models can be optimized for different complexity levels. This approach is much faster and less error-prone than manually creating dependency files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHALLENGE 1: ENHANCED REQUIREMENTS GENERATION\n",
      "============================================================\n",
      "\n",
      "--- STEP 1: ENHANCING REQUIREMENTS PROMPT ---\n",
      "Enhanced requirements prompt:\n",
      "--------------------------------------------------\n",
      "```xml\n",
      "<persona>\n",
      "You are an expert Python developer and a meticulous dependency management specialist. Your task is to analyze Python code and accurately generate a `requirements.txt` file, adhering to best practices for dependency specification.\n",
      "</persona>\n",
      "\n",
      "<context>\n",
      "The user has provided a Python ...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- STEP 2: GENERATING REQUIREMENTS ---\n",
      "Using gemini-2.5-flash (google) for dependency analysis...\n",
      "Generated requirements.txt:\n",
      "--------------------------------------------------\n",
      "fastapi==0.111.0\n",
      "pydantic==2.7.1\n",
      "pytest==8.2.0\n",
      "sqlalchemy==2.0.30\n",
      "uvicorn==0.29.0\n",
      "--------------------------------------------------\n",
      "✅ Requirements.txt saved successfully!\n",
      "\n",
      "============================================================\n",
      "CHALLENGE 1 COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Challenge 1: Enhanced Requirements Generation\n",
    "print(\"=\" * 60)\n",
    "print(\"CHALLENGE 1: ENHANCED REQUIREMENTS GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if app_code:\n",
    "    # Step 1: Create and enhance the raw prompt\n",
    "    print(\"\\n--- STEP 1: ENHANCING REQUIREMENTS PROMPT ---\")\n",
    "    raw_requirements_prompt = f\"\"\"Analyze the following Python code and generate a requirements.txt file listing all the external libraries imported.\n",
    "\n",
    "Include versions for key libraries like fastapi, uvicorn, sqlalchemy, and pydantic. Also include pytest for running tests.\n",
    "\n",
    "--- PYTHON CODE ---\n",
    "{app_code}\n",
    "--- END CODE ---\n",
    "\n",
    "Output only the contents of the requirements.txt file.\"\"\"\n",
    "\n",
    "    enhanced_requirements_prompt = prompt_enhancer(\n",
    "        raw_requirements_prompt, \n",
    "        deps_model_name, \n",
    "        deps_client, \n",
    "        deps_api_provider\n",
    "    )\n",
    "    print(\"Enhanced requirements prompt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(enhanced_requirements_prompt[:300] + \"...\" if len(enhanced_requirements_prompt) > 300 else enhanced_requirements_prompt)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Step 2: Generate requirements using specialized client\n",
    "    print(\"\\n--- STEP 2: GENERATING REQUIREMENTS ---\")\n",
    "    print(f\"Using {deps_model_name} ({deps_api_provider}) for dependency analysis...\")\n",
    "    \n",
    "    requirements_content = get_completion(\n",
    "        enhanced_requirements_prompt, \n",
    "        deps_client, \n",
    "        deps_model_name, \n",
    "        deps_api_provider,\n",
    "        temperature=0.2  # Lower temperature for consistent parsing\n",
    "    )\n",
    "    \n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(\"Generated requirements.txt:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(cleaned_reqs)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "    print(\"✅ Requirements.txt saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ Skipping requirements generation because app code is missing.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHALLENGE 1 COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Explanation:**\n",
    "This challenge showcases how prompt enhancement elevates infrastructure-as-code generation to professional standards. We transform a basic Dockerfile request into a comprehensive DevOps specification that incorporates security best practices, performance optimizations, and production-readiness guidelines.\n",
    "\n",
    "The enhancement process ensures our prompt:\n",
    "- Establishes expert DevOps persona with containerization specialization\n",
    "- Explicitly requests security features (non-root user, multi-stage builds)\n",
    "- Defines performance optimizations (slim base images, layer caching)\n",
    "- Provides structured output requirements with clear technical specifications\n",
    "\n",
    "We use GPT-4o for this intermediate task, as it requires balancing multiple concerns (security, performance, maintainability) while generating syntactically correct Dockerfile syntax. This represents the sweet spot between capability and cost-effectiveness for infrastructure generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHALLENGE 2: ENHANCED DOCKERFILE GENERATION\n",
      "============================================================\n",
      "\n",
      "--- STEP 1: ENHANCING DOCKERFILE PROMPT ---\n",
      "Enhanced Dockerfile prompt:\n",
      "--------------------------------------------------\n",
      "```plaintext\n",
      "<persona>\n",
      "You are an expert Dockerfile developer specializing in creating efficient, secure, and production-ready Dockerfiles for Python applications.\n",
      "</persona>\n",
      "\n",
      "<context>\n",
      "You need to create a multi-stage Dockerfile for a Python FastAPI application. The application is intended for production use, so it must be optimized for minimal image size and efficient layer caching. Security bes...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- STEP 2: GENERATING DOCKERFILE ---\n",
      "Using gpt-4o (openai) for Docker configuration...\n",
      "Generated Dockerfile:\n",
      "--------------------------------------------------\n",
      "# First stage: Build\n",
      "FROM python:3.11-slim AS builder\n",
      "\n",
      "# Set working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Install build dependencies\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends gcc libffi-dev && \\\n",
      "    rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Copy requirements.txt and install dependencies\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Second stage: Production\n",
      "FROM python:3.11-slim\n",
      "\n",
      "# Set working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Create a non-root user\n",
      "RUN useradd -m appuser\n",
      "\n",
      "# Copy application code and installed dependencies from the builder stage\n",
      "COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n",
      "COPY --from=builder /usr/local/bin /usr/local/bin\n",
      "COPY . .\n",
      "\n",
      "# Change ownership of the application code to the non-root user\n",
      "RUN chown -R appuser:appuser /app\n",
      "\n",
      "# Switch to non-root user\n",
      "USER appuser\n",
      "\n",
      "# Expose port 8000\n",
      "EXPOSE 8000\n",
      "\n",
      "# Command to run the application\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
      "--------------------------------------------------\n",
      "✅ Dockerfile saved successfully!\n",
      "\n",
      "============================================================\n",
      "CHALLENGE 2 COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Challenge 2: Enhanced Dockerfile Generation\n",
    "print(\"=\" * 60)\n",
    "print(\"CHALLENGE 2: ENHANCED DOCKERFILE GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create and enhance the raw Dockerfile prompt\n",
    "print(\"\\n--- STEP 1: ENHANCING DOCKERFILE PROMPT ---\")\n",
    "raw_dockerfile_prompt = \"\"\"Generate a best-practice, multi-stage Dockerfile for a production Python FastAPI application.\n",
    "\n",
    "The Dockerfile must:\n",
    "1. Use python:3.11-slim as the base image.\n",
    "2. The first stage should install dependencies from requirements.txt.\n",
    "3. The final stage should copy the application code and the installed dependencies from the first stage.\n",
    "4. Expose port 8000.\n",
    "5. The final CMD should run the application using uvicorn, binding to host 0.0.0.0.\n",
    "6. Follow security best practices including running as non-root user.\n",
    "7. Optimize for minimal image size and efficient layer caching.\n",
    "\n",
    "Output only the raw Dockerfile content.\"\"\"\n",
    "\n",
    "enhanced_dockerfile_prompt = prompt_enhancer(\n",
    "    raw_dockerfile_prompt, \n",
    "    docker_model_name, \n",
    "    docker_client, \n",
    "    docker_api_provider\n",
    ")\n",
    "print(\"Enhanced Dockerfile prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(enhanced_dockerfile_prompt[:400] + \"...\" if len(enhanced_dockerfile_prompt) > 400 else enhanced_dockerfile_prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 2: Generate Dockerfile using specialized client\n",
    "print(\"\\n--- STEP 2: GENERATING DOCKERFILE ---\")\n",
    "print(f\"Using {docker_model_name} ({docker_api_provider}) for Docker configuration...\")\n",
    "\n",
    "dockerfile_content = get_completion(\n",
    "    enhanced_dockerfile_prompt, \n",
    "    docker_client, \n",
    "    docker_model_name, \n",
    "    docker_api_provider,\n",
    "    temperature=0.3  # Slightly higher for creative infrastructure solutions\n",
    ")\n",
    "\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(\"Generated Dockerfile:\")\n",
    "print(\"-\" * 50)\n",
    "print(cleaned_dockerfile)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")\n",
    "    print(\"✅ Dockerfile saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ Failed to generate valid Dockerfile content.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHALLENGE 2 COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Explanation:**\n",
    "This challenge represents the pinnacle of Configuration-as-Code generation, where we transform a basic CI/CD request into a comprehensive, production-ready automation pipeline. The prompt enhancement process applies sophisticated engineering principles to ensure the generated workflow adheres to industry best practices.\n",
    "\n",
    "The enhancement incorporates:\n",
    "- **Expert CI/CD Specialist Persona**: Establishes deep domain expertise in automation pipelines\n",
    "- **Comprehensive Context Grounding**: Provides detailed understanding of GitHub Actions ecosystem\n",
    "- **Chain-of-Thought Integration**: Guides the model through systematic workflow design decisions\n",
    "- **Strict Output Validation**: Ensures syntactically correct YAML with proper schema compliance\n",
    "- **Security and Performance Considerations**: Integrates caching, parallelization, and secure practices\n",
    "\n",
    "We use DeepSeek-V3.1 for this advanced task because it excels at complex, multi-step reasoning required for sophisticated workflow orchestration. This model's architectural strengths in logical sequencing and dependency management make it ideal for generating robust CI/CD pipelines that can handle real-world complexity and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHALLENGE 3: ENHANCED CI/CD WORKFLOW GENERATION\n",
      "============================================================\n",
      "\n",
      "--- STEP 1: ENHANCING CI/CD WORKFLOW PROMPT ---\n",
      "Enhanced CI/CD workflow prompt:\n",
      "--------------------------------------------------\n",
      "<persona>\n",
      "You are a Senior DevOps Engineer and GitHub Actions specialist with deep expertise in creating robust, efficient, and secure CI/CD pipelines for Python applications. Your knowledge of caching strategies, dependency management, and testing best practices is authoritative.\n",
      "</persona>\n",
      "\n",
      "<context>\n",
      "The task is to generate a complete GitHub Actions workflow YAML file for a Python FastAPI project. The workflow must automate the processes of building, testing, and performing basic code quality ...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- STEP 2: GENERATING CI/CD WORKFLOW ---\n",
      "Using deepseek-ai/DeepSeek-V3.1 (huggingface) for advanced workflow orchestration...\n",
      "Generated GitHub Actions Workflow:\n",
      "--------------------------------------------------\n",
      "name: Python CI\n",
      "\n",
      "on:\n",
      "  push:\n",
      "    branches: [ main ]\n",
      "  pull_request:\n",
      "    branches: [ main ]\n",
      "\n",
      "jobs:\n",
      "  build-and-test:\n",
      "    runs-on: ubuntu-latest\n",
      "\n",
      "    steps:\n",
      "    - name: Checkout code\n",
      "      uses: actions/checkout@v4\n",
      "\n",
      "    - name: Set up Python 3.11\n",
      "      uses: actions/setup-python@v5\n",
      "      with:\n",
      "        python-version: \"3.11\"\n",
      "\n",
      "    - name: Cache dependencies\n",
      "      uses: actions/cache@v3\n",
      "      with:\n",
      "        path: ~/.cache/pip\n",
      "        key: ${{ runner.os }}-python-${{ env.pythonVersion }}-pip-${{ hashFiles('requirements.txt') }}\n",
      "        restore-keys: |\n",
      "          ${{ runner.os }}-python-${{ env.pythonVersion }}-pip-\n",
      "\n",
      "    - name: Install dependencies\n",
      "      run: pip install -r requirements.txt\n",
      "\n",
      "    - name: Run tests with pytest\n",
      "      run: pytest -v\n",
      "\n",
      "    - name: Lint with ruff\n",
      "      run: |\n",
      "        ruff check .\n",
      "        ruff format --check .\n",
      "--------------------------------------------------\n",
      "✅ GitHub Actions workflow saved successfully!\n",
      "\n",
      "============================================================\n",
      "CHALLENGE 3 COMPLETED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Challenge 3: Enhanced GitHub Actions Workflow Generation\n",
    "print(\"=\" * 60)\n",
    "print(\"CHALLENGE 3: ENHANCED CI/CD WORKFLOW GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Create and enhance the raw CI/CD workflow prompt\n",
    "print(\"\\n--- STEP 1: ENHANCING CI/CD WORKFLOW PROMPT ---\")\n",
    "raw_cicd_prompt = \"\"\"Generate a complete GitHub Actions workflow file named ci.yml for a Python FastAPI project.\n",
    "\n",
    "The workflow must:\n",
    "- Be named 'Build and Test'.\n",
    "- Trigger on every push to the main branch and on pull requests.\n",
    "- Define one job named build-and-test that runs on ubuntu-latest.\n",
    "- The job must have the following sequential steps:\n",
    "  1. actions/checkout@v4 to check out the repository code.\n",
    "  2. actions/setup-python@v5 to set up Python 3.11.\n",
    "  3. A step to cache pip dependencies for faster builds.\n",
    "  4. A step to install dependencies using pip from requirements.txt.\n",
    "  5. A step to run the test suite using pytest with verbose output.\n",
    "  6. A step to run basic linting/formatting checks.\n",
    "\n",
    "Include proper error handling, environment variables, and follow GitHub Actions best practices.\n",
    "Output only the raw YAML content for the file.\"\"\"\n",
    "\n",
    "enhanced_cicd_prompt = prompt_enhancer(\n",
    "    raw_cicd_prompt, \n",
    "    cicd_model_name, \n",
    "    cicd_client, \n",
    "    cicd_api_provider\n",
    ")\n",
    "print(\"Enhanced CI/CD workflow prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(enhanced_cicd_prompt[:500] + \"...\" if len(enhanced_cicd_prompt) > 500 else enhanced_cicd_prompt)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Step 2: Generate CI/CD workflow using specialized client\n",
    "print(\"\\n--- STEP 2: GENERATING CI/CD WORKFLOW ---\")\n",
    "print(f\"Using {cicd_model_name} ({cicd_api_provider}) for advanced workflow orchestration...\")\n",
    "\n",
    "ci_workflow_content = get_completion(\n",
    "    enhanced_cicd_prompt, \n",
    "    cicd_client, \n",
    "    cicd_model_name, \n",
    "    cicd_api_provider,\n",
    "    temperature=0.2  # Low temperature for consistent, reliable automation\n",
    ")\n",
    "\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(\"Generated GitHub Actions Workflow:\")\n",
    "print(\"-\" * 50)\n",
    "print(cleaned_yaml)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")\n",
    "    print(\"✅ GitHub Actions workflow saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ Failed to generate valid workflow content.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHALLENGE 3 COMPLETED!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Outstanding! You have successfully demonstrated the power of **Enhanced AI-Driven DevOps** by generating a complete, enterprise-grade Continuous Integration pipeline using advanced prompt engineering techniques and strategic model selection.\n",
    "\n",
    "### Key Innovations Demonstrated:\n",
    "\n",
    "1. **Prompt Enhancement Engineering**: Transformed basic requests into sophisticated, production-ready specifications using our `prompt_enhancer` meta-system that applies expert personas, contextual grounding, and structured output requirements.\n",
    "\n",
    "2. **Strategic Model Selection**: \n",
    "   - **Gemini Flash** for fast dependency parsing (foundational tasks)\n",
    "   - **GPT-4o** for balanced infrastructure generation (intermediate complexity)\n",
    "   - **DeepSeek-V3.1** for complex workflow orchestration (advanced reasoning)\n",
    "\n",
    "3. **Configuration-as-Code Mastery**: Generated three critical DevOps artifacts:\n",
    "   - `requirements.txt` (dependency management)\n",
    "   - `Dockerfile` (containerization with security best practices)\n",
    "   - `ci.yml` (automated testing and integration pipeline)\n",
    "\n",
    "### Professional Impact:\n",
    "\n",
    "This lab demonstrates how AI can elevate DevOps practices from manual, error-prone processes to automated, consistent, and secure infrastructure generation. By combining prompt enhancement with strategic model selection, you've created a scalable approach that can be applied across different CI/CD complexity levels while maintaining cost-effectiveness and quality.\n",
    "\n",
    "> **Key Takeaway:** The integration of prompt enhancement with specialized model selection represents the next evolution in AI-assisted DevOps. This approach enables teams to generate sophisticated, production-ready infrastructure configurations that incorporate security best practices, performance optimizations, and industry standards—all while maintaining the flexibility to optimize for different complexity levels and cost considerations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
