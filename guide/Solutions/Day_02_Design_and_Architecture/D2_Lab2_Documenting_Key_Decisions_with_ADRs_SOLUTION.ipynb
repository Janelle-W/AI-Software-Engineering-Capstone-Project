{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs (Solution)\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for the ADR generation lab. It demonstrates how to use an LLM for comparative research and then synthesize that research into a structured, formal document.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-21 21:13:25,422 ag_aisoftdev.utils INFO LLM Client configured provider=huggingface model=deepseek-ai/DeepSeek-V3.1 latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:13:25,704 ag_aisoftdev.utils INFO LLM Client configured provider=anthropic model=claude-opus-4-1-20250805 latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:13:25,863 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-5-2025-08-07 latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different tasks using models from different providers.\n",
    "# - Template generation: use a HuggingFace instruction-following model\n",
    "# - Research/comparison: use an Anthropic model\n",
    "# - Synthesis (final ADR): use another HuggingFace model to demonstrate multi-provider usage\n",
    "template_client, template_model_name, template_api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "research_client, research_model_name, research_api_provider = setup_llm_client(model_name=\"claude-opus-4-1-20250805\")\n",
    "synthesis_client, synthesis_model_name, synthesis_api_provider = setup_llm_client(model_name=\"gpt-5-2025-08-07\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks the LLM to generate a standard markdown template for an ADR. The key is to be specific about the sections required (`Title`, `Status`, `Context`, `Decision`, `Consequences`), which guides the LLM to produce a well-structured and reusable template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:14:34,860 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "```markdown\n",
      "# Title: [A short, descriptive title for the decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
      "\n",
      "## Context\n",
      "- [Describe the problem, the driving forces, and the constraints.]\n",
      "\n",
      "## Decision\n",
      "- [State the chosen solution clearly and concisely.]\n",
      "\n",
      "## Consequences\n",
      "- [List the positive outcomes, negative trade-offs, and any future work required.]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "adr_template_prompt = \"\"\"You are a principal engineer who champions clear documentation. Generate a concise, reusable markdown template for an Architectural Decision Record (ADR).\n",
    "\n",
    "The template must include the following sections:\n",
    "- # Title: [A short, descriptive title for the decision]\n",
    "- **Status:** [Proposed | Accepted | Deprecated | Superseded]\n",
    "- ## Context\n",
    "  - [Describe the problem, the driving forces, and the constraints.]\n",
    "- ## Decision\n",
    "  - [State the chosen solution clearly and concisely.]\n",
    "- ## Consequences\n",
    "  - [List the positive outcomes, negative trade-offs, and any future work required.]\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "enhanced_adr_template_prompt = prompt_enhancer(adr_template_prompt)\n",
    "adr_template_content = get_completion(enhanced_adr_template_prompt, template_client, template_model_name, template_api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Explanation:**\n",
    "This prompt leverages the LLM's vast training data to perform a comparative analysis. By instructing it to be an \"unbiased research assistant\" and asking for \"pros and cons for each,\" we guide the model to provide a balanced view rather than a simple recommendation. This produces a more valuable and objective input for our own decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:14:44,772 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "### Comparative Analysis\n",
      "\n",
      "| Factor | PostgreSQL + pgvector – Pros | PostgreSQL + pgvector – Cons | Dedicated Vector DB – Pros | Dedicated Vector DB – Cons |\n",
      "|--------|-----------------------------|-----------------------------|---------------------------|----------------------------|\n",
      "| **Operational Complexity** | • Single database to manage, backup, and monitor<br>• Existing PostgreSQL expertise applies<br>• Unified security model and access controls<br>• Simple disaster recovery with standard PostgreSQL tools | • Requires PostgreSQL tuning knowledge for vector workloads<br>• Index rebuilding can block operations<br>• Limited vector-specific monitoring tools | • Purpose-built admin interfaces for vector operations<br>• Automated index optimization<br>• Built-in vector-specific monitoring dashboards | • Additional system to operate and secure<br>• Separate backup/recovery procedures needed<br>• New technology stack for team to learn<br>• Complex data synchronization between systems |\n",
      "| **Cost** | • No additional licensing fees (open source)<br>• Shared infrastructure with existing PostgreSQL<br>• Single managed service possible (RDS, Cloud SQL)<br>• No data transfer costs between systems | • Higher compute requirements for mixed workloads<br>• Potential need for larger instances<br>• Storage less optimized for vectors | • Optimized storage reduces infrastructure costs<br>• Some offer generous free tiers (ChromaDB)<br>• Efficient resource utilization for vector operations | • Additional infrastructure costs<br>• Separate licensing/subscription fees<br>• Data transfer costs between services<br>• Potential vendor lock-in with commercial options |\n",
      "| **Query Flexibility** | • Full SQL capabilities for hybrid queries<br>• JOIN vectors with relational data natively<br>• Complex filtering before/after similarity search<br>• Transactional consistency across all data | • Limited vector search algorithms (IVFFlat, HNSW)<br>• Fewer distance metrics than specialized solutions<br>• No built-in re-ranking capabilities | • Multiple indexing algorithms available<br>• Advanced features: hybrid search, re-ranking, filtering<br>• Richer distance metric options<br>• Built-in metadata filtering optimizations | • Limited relational query capabilities<br>• Requires application-level JOINs<br>• Complex queries need multiple roundtrips<br>• Eventual consistency challenges with PostgreSQL data |\n",
      "| **Scalability & Performance** | • Adequate for 1-10M vectors with proper tuning<br>• Predictable performance with HNSW index<br>• Vertical scaling straightforward<br>• Query latency <300ms achievable for moderate QPS | • Horizontal scaling limited to read replicas<br>• Performance degrades with concurrent mixed workloads<br>• Memory-intensive for large vector dimensions<br>• Index build times increase significantly >5M vectors | • Superior performance at scale<br>• Horizontal scaling built-in<br>• Optimized memory management for vectors<br>• Sub-100ms latencies common<br>• Efficient batch operations | • Over-engineered for small datasets<br>• Performance benefits negligible <1M vectors<br>• Requires careful capacity planning<br>• Network latency adds overhead for small queries |\n",
      "\n",
      "### Recommendation\n",
      "\n",
      "For small-to-medium enterprises with 1-10M embeddings and moderate QPS, **PostgreSQL + pgvector** is recommended when: relational data integration is critical, operational simplicity is prioritized, and the team has existing PostgreSQL expertise. The unified stack reduces complexity and costs while meeting performance requirements.\n",
      "\n",
      "Choose a **dedicated vector database** when: planning significant growth beyond 10M vectors, requiring advanced vector search features (hybrid search, re-ranking), operating vector search as a separate microservice, or when PostgreSQL hosts critical transactional workloads that shouldn't compete with vector operations. The additional complexity is justified only when specialized features or scale demands exceed pgvector's capabilities.\n"
     ]
    }
   ],
   "source": [
    "db_research_prompt = \"\"\"You are an unbiased research assistant. Your task is to provide a balanced technical comparison for a software development team.\n",
    "\n",
    "For the use case of a new hire onboarding tool that needs a semantic search feature, compare and contrast the following two approaches:\n",
    "\n",
    "1.  **Approach 1:** Using PostgreSQL with the `pgvector` extension.\n",
    "2.  **Approach 2:** Using a specialized, dedicated vector database (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "Please provide a summary of the pros and cons for each approach. Consider factors like operational complexity, cost, query flexibility, and scalability for a small-to-medium sized enterprise application.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "enhanced_db_research_prompt = prompt_enhancer(db_research_prompt)\n",
    "db_research_output = get_completion(enhanced_db_research_prompt, research_client, research_model_name, research_api_provider)\n",
    "print(db_research_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Explanation:**\n",
    "This prompt demonstrates a powerful synthesis task. We provide the LLM with two key inputs: unstructured information (the research) and a desired structure (the template). The agent's job is to merge them, creating a polished, formal document. This is a repeatable pattern for turning raw analysis into professional documentation. By assigning the persona of a Staff Engineer, we guide the LLM to adopt a formal and authoritative tone suitable for an official project artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:15:22,062 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "```markdown\n",
      "# Title: Use PostgreSQL with pgvector for Vector Search and Storage\n",
      "\n",
      "**Status:** Accepted\n",
      "\n",
      "## Context\n",
      "- We need vector similarity search tightly integrated with relational data, and must choose between PostgreSQL with pgvector and a dedicated vector database.\n",
      "- Operational simplicity and cost control are priorities; we prefer to avoid new licenses, vendors, and parallel operational stacks while leveraging existing PostgreSQL expertise and security.\n",
      "- Target workload is small-to-medium scale (approximately 1–10 million embeddings) with acceptable latency under ~300 ms; the ability to perform native SQL joins alongside vector queries is important.\n",
      "- Research indicates PostgreSQL + pgvector provides single-stack simplicity, unified security, and native SQL joins with vectors, delivering adequate performance at the targeted scale after tuning.\n",
      "- Risks include limited horizontal scaling, fewer vector search algorithms, heavier compute demands under mixed workloads, and longer index-build times beyond ~5 million vectors.\n",
      "- Dedicated vector databases excel at >10 million vectors, advanced search features, and sub-100 ms latency but add operational overhead, extra cost, and potential vendor lock-in.\n",
      "\n",
      "## Decision\n",
      "- We will use PostgreSQL with the pgvector extension as our vector store and similarity search engine for this project.\n",
      "\n",
      "## Consequences\n",
      "- Single-stack operations with no new licenses, leveraging existing PostgreSQL tooling, expertise, and unified security/governance.\n",
      "- Native SQL joins with vectors simplify application logic and enable flexible query composition across relational and vector data.\n",
      "- Meets current scale and performance needs (≈1–10 million embeddings, <300 ms latency) with appropriate tuning and indexing.\n",
      "- Limited horizontal scalability and fewer advanced vector features may constrain future needs (e.g., sub-100 ms latency or >10 million vectors).\n",
      "- Mixed OLTP and vector workloads can drive higher compute requirements; index builds may become lengthy beyond ~5 million vectors and require careful maintenance windows.\n",
      "- Ongoing performance monitoring and benchmarking are required to track latency/recall as data grows and workloads evolve.\n",
      "- Define migration criteria and a contingency plan for adopting a dedicated vector database if scale, latency, or feature requirements surpass PostgreSQL + pgvector capabilities.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "synthesis_prompt = f\"\"\"You are a Staff Engineer responsible for documenting key architectural decisions.\n",
    "\n",
    "Your task is to populate the provided ADR template to formally document the decision to **use PostgreSQL with the pgvector extension** for our project.\n",
    "\n",
    "Use the research provided below to fill in the 'Context' and 'Consequences' sections of the template. Be thorough and objective, summarizing the key points from the research.\n",
    "\n",
    "--- ADR TEMPLATE ---\n",
    "{adr_template}\n",
    "--- END TEMPLATE ---\n",
    "\n",
    "--- RESEARCH CONTEXT ---\n",
    "{db_research_output}\n",
    "--- END CONTEXT ---\n",
    "\n",
    "The final ADR should be complete and ready for review.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    enhanced_synthesis_prompt = prompt_enhancer(synthesis_prompt)\n",
    "    final_adr = get_completion(enhanced_synthesis_prompt, synthesis_client, synthesis_model_name, synthesis_api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
