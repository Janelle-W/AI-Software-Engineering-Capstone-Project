{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding (Solution)\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and Python code for Day 2's first lab. It demonstrates the workflow of generating design artifacts (SQL schema, seed data) and then using code to create a physical database from them.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the `day1_prd.md` artifact from Day 1. This document is the single source of truth for our project's requirements and provides the essential context for the LLM to generate a relevant and accurate database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 20:01:34,600 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n",
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-21 20:01:34,825 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - Schema generation uses a strong instruction-following model\n",
    "# - Seed data generation uses a model tuned for data generation\n",
    "schema_client, schema_model_name, schema_api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "seed_client, seed_model_name, seed_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"day1_prd.md\")\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load day1_prd.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\n",
       "| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\n",
       "| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| Qwen/Qwen-Image | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| Qwen/Qwen-Image-Edit | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| black-forest-labs/FLUX.1-Kontext-dev | huggingface | ❌ | ❌ | ❌ | ✅ | ❌ | - | - |\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| dall-e-3 | openai | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-preview-image-generation | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,000 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ❌ | ❌ | ✅ | ❌ | ❌ | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-live-2.5-flash-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| stabilityai/stable-diffusion-3.5-large | huggingface | ❌ | ❌ | ✅ | ❌ | ❌ | - | - |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |\\n| veo-3.0-fast-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| veo-3.0-generate-preview | google | ❌ | ❌ | ❌ | ❌ | ❌ | 1,024 | - |\\n| whisper-1 | openai | ❌ | ❌ | ❌ | ❌ | ✅ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Explanation:**\n",
    "This prompt instructs the LLM to act as a Database Administrator (DBA). By providing the full PRD as context, we enable the LLM to understand the entities and relationships required by the application. The prompt specifically asks for `CREATE TABLE` statements, guiding the LLM to produce a ready-to-use SQL script. We then clean up the response to remove markdown fences and save the pure SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 20:01:34,852 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "Schema Enhanced prompt\n",
      " <prompt>\n",
      "  <persona>\n",
      "    You are an expert Database Administrator (DBA) specializing in relational data-modeling, normalization (to 3rd Normal Form), and SQLite syntax.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    Product: “Catalyst Onboarding Platform” (centralized new-hire onboarding web app).\n",
      "\n",
      "    Core data requirements for Version 1.0 (MVP):\n",
      "      • Store user records.  \n",
      "      • Store onboarding tasks assigned to users.  \n",
      "      • Each task belongs to exactly one user; each user can have many tasks.  \n",
      "      • Track task status (“Pending”, “Completed”), due dates, and descriptive metadata.\n",
      "\n",
      "    Mandatory table/column specs:\n",
      "      1. users\n",
      "         – id (PK, integer, auto-increment)  \n",
      "         – name (text)  \n",
      "         – email (text, unique)  \n",
      "         – role (text)  -- e.g., 'New Hire', 'Manager'\n",
      "      2. onboarding_tasks\n",
      "         – id (PK, integer, auto-increment)  \n",
      "         – title (text)  \n",
      "         – description (text)  \n",
      "         – due_date (date)  \n",
      "         – status (text)  -- e.g., 'Pending', 'Completed'  \n",
      "         – user_id (FK → users.id, cascade on delete)\n",
      "\n",
      "    Additional constraints:\n",
      "      • Use only standard SQLite features.  \n",
      "      • Enforce referential integrity with FOREIGN KEY clauses.  \n",
      "      • Apply reasonable NOT NULL and CHECK constraints.  \n",
      "      • Normalize to avoid redundancy (3NF).  \n",
      "      • Output must consist solely of valid SQL CREATE TABLE statements—no comments or prose.\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "    1. Think step-by-step (internally) to design a clean, normalized schema that satisfies all constraints.  \n",
      "    2. After reasoning, output ONLY the final SQL CREATE TABLE statements in correct execution order.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "```sql\n",
      "-- SQL statements here\n",
      "```\n",
      "  </output_format>\n",
      "</prompt>\n",
      "CREATE TABLE users (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    role TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager'))\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    title TEXT NOT NULL,\n",
      "    description TEXT NOT NULL,\n",
      "    due_date DATE NOT NULL,\n",
      "    status TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "    user_id INTEGER NOT NULL,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "schema_prompt = f\"\"\"\n",
    "You are an expert Database Administrator (DBA).\n",
    "\n",
    "Based on the following Product Requirements Document (PRD), design a normalized SQL schema for a SQLite database. The schema should include tables for users and their assigned onboarding tasks.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "The schema should have at least a `users` table and an `onboarding_tasks` table with a foreign key relationship.\n",
    "- The `users` table should include an id, name, email, and role (e.g., 'New Hire', 'Manager').\n",
    "- The `onboarding_tasks` table should include an id, a title, a description, a due_date, a status (e.g., 'Pending', 'Completed'), and a user_id foreign key.\n",
    "\n",
    "Output only the raw SQL `CREATE TABLE` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    # Enhance the raw schema prompt using the project's prompt enhancer\n",
    "    enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "    print(\"Schema Enhanced prompt\\n\", enhanced_schema_prompt)\n",
    "\n",
    "    # Send the enhanced prompt to the schema-specific LLM client\n",
    "    generated_schema = get_completion(enhanced_schema_prompt, schema_client, schema_model_name, schema_api_provider)\n",
    "\n",
    "    # Clean up the generated schema\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "\n",
    "    # Save the cleaned schema to a file\n",
    "    save_artifact(cleaned_schema, \"artifacts/schema.sql\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Explanation:**\n",
    "An empty database isn't very useful for development. This prompt asks the LLM to generate realistic seed data. By providing both the PRD (for thematic context) and the SQL schema (for structural correctness), we guide the LLM to create `INSERT` statements that are both thematically appropriate (e.g., onboarding-related task titles) and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 20:01:49,004 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "Seed Data Enhanced prompt\n",
      " <prompt>\n",
      "  <persona>\n",
      "    You are a Senior Data Specialist with deep experience in relational database design and SQL scripting. Your task is to generate clean, syntactically correct SQL `INSERT` statements that could be executed without modification in a SQLite-compatible RDBMS.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    Below are two reference blocks you must ground your answer in:\n",
      "\n",
      "    1. <prd>\n",
      "       (Full PRD content exactly as supplied by the user)\n",
      "    </prd>\n",
      "\n",
      "    2. <schema>\n",
      "       CREATE TABLE users (\n",
      "           id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "           name TEXT NOT NULL,\n",
      "           email TEXT NOT NULL UNIQUE,\n",
      "           role TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager'))\n",
      "       );\n",
      "\n",
      "       CREATE TABLE onboarding_tasks (\n",
      "           id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "           title TEXT NOT NULL,\n",
      "           description TEXT NOT NULL,\n",
      "           due_date DATE NOT NULL,\n",
      "           status TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "           user_id INTEGER NOT NULL,\n",
      "           FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n",
      "       );\n",
      "    </schema>\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "    1. Think through the required foreign-key relationships so that every task’s `user_id` references an existing user ID.  \n",
      "    2. Generate between 8 and 12 total `INSERT` statements, covering:\n",
      "       • At least 3 distinct users (rows for the `users` table).  \n",
      "       • At least 5 distinct tasks (rows for the `onboarding_tasks` table).  \n",
      "    3. Make the sample data realistic and thematically tied to the onboarding platform described in the PRD (e.g., “Complete welcome microsite”).  \n",
      "    4. Assume IDs start at 1 and auto-increment; supply explicit ID values to make the relationships clear.  \n",
      "    5. Do NOT output any commentary, explanation, or code fences—only the raw SQL `INSERT` statements separated by newlines.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "INSERT INTO users (id, name, email, role) VALUES (...);\n",
      "INSERT INTO users (id, name, email, role) VALUES (...);\n",
      "...\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (...);\n",
      "...\n",
      "  </output_format>\n",
      "</prompt>\n",
      "INSERT INTO users (id, name, email, role) VALUES (1, 'Alice Johnson', 'alice.j@example.com', 'Manager');\n",
      "INSERT INTO users (id, name, email, role) VALUES (2, 'Bob Smith', 'bob.s@example.com', 'New Hire');\n",
      "INSERT INTO users (id, name, email, role) VALUES (3, 'Charlie Brown', 'charlie.b@example.com', 'New Hire');\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (1, 'Complete HR Paperwork', 'Fill out and submit all required tax and employment forms online via the HR portal.', '2024-08-05', 'Completed', 2);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (2, 'Set Up Development Environment', 'Install required software and configure your local machine as per the team guide.', '2024-08-07', 'Pending', 2);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (3, 'Meet Your Mentor', 'Schedule a 30-minute introductory meeting with your assigned mentor, Alice Johnson.', '2024-08-09', 'Pending', 2);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (4, 'Complete HR Paperwork', 'Fill out and submit all required tax and employment forms online via the HR portal.', '2024-08-05', 'Completed', 3);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (5, 'Security Awareness Training', 'Complete the mandatory online security awareness course and quiz.', '2024-08-08', 'Completed', 3);\n",
      "INSERT INTO onboarding_tasks (id, title, description, due_date, status, user_id) VALUES (6, 'Review Team Project Board', 'Familiarize yourself with the current sprint tasks and project backlog on our Jira board.', '2024-08-10', 'Pending', 3);\n"
     ]
    }
   ],
   "source": [
    "seed_data_prompt = f\"\"\"\n",
    "You are a data specialist. Based on the provided PRD and SQL schema, generate 5-10 realistic SQL `INSERT` statements to populate the tables with sample data for an onboarding tool.\n",
    "\n",
    "**PRD Context:**\n",
    "<prd>\n",
    "{prd_content}\n",
    "</prd>\n",
    "\n",
    "**SQL Schema:**\n",
    "<schema>\n",
    "{cleaned_schema}\n",
    "</schema>\n",
    "\n",
    "Generate at least 3 users and 5 tasks assigned to those users.\n",
    "Output only the raw SQL `INSERT` statements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    # Enhance the seed data prompt for better structure and fidelity\n",
    "    enhanced_seed_prompt = prompt_enhancer(seed_data_prompt)\n",
    "    print(\"Seed Data Enhanced prompt\\n\", enhanced_seed_prompt)\n",
    "\n",
    "    # Use the seed-data specific client\n",
    "    generated_seed_data = get_completion(enhanced_seed_prompt, seed_client, seed_model_name, seed_api_provider)\n",
    "\n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "\n",
    "    # Save the cleaned seed data to a file\n",
    "    save_artifact(cleaned_seed_data, \"artifacts/seed_data.sql\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Explanation:**\n",
    "This Python function demonstrates a crucial engineering task: turning text-based artifacts into a live system component. The `create_database` function uses Python's built-in `sqlite3` library.\n",
    "1.  It establishes a connection to a database file, which creates the file if it doesn't exist.\n",
    "2.  It reads the `schema.sql` artifact and executes it. It's important to use `cursor.executescript()` here. While `cursor.execute()` is designed for a single SQL statement, `executescript()` is necessary for running a string that contains multiple SQL statements, which is exactly what our `schema.sql` and `seed_data.sql` files contain.\n",
    "3.  It then reads and executes the `seed_data.sql` artifact to populate the newly created tables.\n",
    "4.  `conn.commit()` saves all the changes to the database file.\n",
    "5.  The `finally` block ensures that `conn.close()` is always called, which is a critical best practice to prevent resource leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to database at /Users/agaleana/repos/AG-AISOFTDEV/artifacts/onboarding.db\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "\n",
    "    # Delete the old database file if it exists to start fresh\n",
    "    if os.path.exists(db_path):\n",
    "        os.remove(db_path)\n",
    "        print(f\"Removed existing database file at {db_path}\")\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # Read and execute the schema file\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        if schema_sql:\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Read and execute the seed data file if it exists\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "# Define file paths\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"onboarding.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
