{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Lab 1: AI-Driven Backend Development (Solution)\n",
    "\n",
    "**Objective:** Generate a complete FastAPI backend application, including Pydantic and SQLAlchemy models, and then perform the critical engineering task of integrating the generated code with the live SQLite database created on Day 2.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete code and prompts for generating and assembling the database-connected API. It highlights the workflow of generating components separately and then integrating them, a common pattern in AI-assisted development.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load our `schema.sql` artifact, which will be the primary context for our code generation prompts. Having the database schema is essential for the LLM to accurately generate models (both Pydantic and SQLAlchemy) and endpoints that match our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agaleana/repos/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-21 21:27:20,363 ag_aisoftdev.utils INFO LLM Client configured provider=huggingface model=meta-llama/Llama-4-Scout-17B-16E-Instruct latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:27:20,598 ag_aisoftdev.utils INFO LLM Client configured provider=anthropic model=claude-opus-4-1-20250805 latency_ms=None artifacts_path=None\n",
      "2025-09-21 21:27:20,863 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - In-memory app generation: use a Scout/Llama family model for instruction-following code generation\n",
    "# - DB models & session code: use a strong instruction-following model (e.g. gpt-4o)\n",
    "# - Integration/synthesis tasks: use another high-quality model (e.g. gemini-2.5-pro)\n",
    "in_memory_client, in_memory_model_name, in_memory_api_provider = setup_llm_client(model_name=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "db_client, db_model_name, db_api_provider = setup_llm_client(model_name=\"claude-opus-4-1-20250805\")\n",
    "integration_client, integration_model_name, integration_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the SQL schema from Day 2\n",
    "sql_schema = load_artifact(\"artifacts/schema.sql\")\n",
    "if not sql_schema:\n",
    "    print(\"Warning: Could not load schema.sql. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table(text_generation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating Code with In-Memory Logic\n",
    "\n",
    "**Explanation:**\n",
    "This prompt generates a fully functional but simplified version of our application. By asking for an in-memory database, we allow the LLM to focus on generating the correct API structure, endpoints, and Pydantic models without the added complexity of database integration code. This gives us a clean, working baseline that we can build upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:27:21,001 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating FastAPI app with in-memory database ---\n",
      "In-memory Enhanced prompt\n",
      " <prompt>\n",
      "\n",
      "  <persona>\n",
      "You are a senior Python backend engineer with deep expertise in FastAPI and Pydantic.  \n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "A new-hire onboarding tool will initially manage only user resources.  \n",
      "SQL schema excerpt (for reference, don’t replicate in output):\n",
      "\n",
      "```sql\n",
      "CREATE TABLE users (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    role TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager'))\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    title TEXT NOT NULL,\n",
      "    description TEXT NOT NULL,\n",
      "    due_date DATE NOT NULL,\n",
      "    status TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "    user_id INTEGER NOT NULL,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n",
      ");\n",
      "```\n",
      "\n",
      "Feature requirements for the initial FastAPI service (file: `main.py`):\n",
      "\n",
      "1. All necessary FastAPI imports.  \n",
      "2. Pydantic models: a `UserCreate` model (for POST) and a `UserRead` model (for responses) with fields `id`, `name`, `email`, `role`.  \n",
      "3. A simple in-memory list acting as a fake user database.  \n",
      "4. CRUD endpoints under `/users`:  \n",
      "   • POST `/users` to add a user to the list and return the created record.  \n",
      "   • GET `/users` to return all users.  \n",
      "   • GET `/users/{user_id}` to return a user by ID (404 if not found).  \n",
      "5. All endpoint logic must operate exclusively on the in-memory list.  \n",
      "6. No persistence, no external libraries beyond FastAPI’s standard dependencies.  \n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "First, think through the solution steps internally (don’t reveal this reasoning). Then generate a single, self-contained `main.py` script implementing all requirements.  \n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "Output ONLY the raw Python code for `main.py`; do not add comments, explanations, or any other text.  \n",
      "  </output_format>\n",
      "\n",
      "</prompt>\n",
      "Saved in-memory API to app/main_in_memory.py\n"
     ]
    }
   ],
   "source": [
    "in_memory_api_prompt = f\"\"\"\n",
    "You are a senior Python developer creating a FastAPI application for a new hire onboarding tool.\n",
    "\n",
    "Based on the following SQL schema, generate a single Python script for a `main.py` file that includes:\n",
    "1.  All necessary FastAPI imports.\n",
    "2.  Pydantic models for creating and reading `User` resources. Include fields for `id`, `name`, `email`, and `role`.\n",
    "3.  A simple in-memory list to act as a fake database for users.\n",
    "4.  Complete FastAPI CRUD endpoints for the `/users` path (POST, GET all, GET by ID).\n",
    "5.  The endpoints should perform their logic on the in-memory list.\n",
    "\n",
    "**SQL Schema Context:**\n",
    "```sql\n",
    "{sql_schema}\n",
    "```\n",
    "\n",
    "Output only the raw Python code.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating FastAPI app with in-memory database ---\")\n",
    "if sql_schema:\n",
    "    # Enhance the prompt so the model adopts a clear persona and structured output expectations\n",
    "    enhanced_in_memory_api_prompt = prompt_enhancer(in_memory_api_prompt)\n",
    "    print(\"In-memory Enhanced prompt\\n\", enhanced_in_memory_api_prompt)\n",
    "    generated_api_code = get_completion(enhanced_in_memory_api_prompt, in_memory_client, in_memory_model_name, in_memory_api_provider)\n",
    "    cleaned_code = clean_llm_output(generated_api_code, language='python')\n",
    "    # Save this code to a temporary reference file\n",
    "    save_artifact(cleaned_code, \"app/main_in_memory.py\", overwrite=True)\n",
    "    print(\"Saved in-memory API to app/main_in_memory.py\")\n",
    "else:\n",
    "    print(\"Skipping API generation because schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Database Models and Session Code\n",
    "\n",
    "**Explanation:**\n",
    "This prompt is highly specific. It asks for the two key components needed for database connectivity in a modern Python application: the ORM (Object-Relational Mapping) models and the session management code. \n",
    "-   **SQLAlchemy Models:** These classes map our Python objects directly to the tables in our database, allowing us to work with Python code instead of raw SQL.\n",
    "-   **Session Management:** This is the standard FastAPI pattern for handling database connections. The `get_db` function is a dependency that ensures each API request gets a database session and that the session is properly closed afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:27:31,890 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQLAlchemy Models and Session Code ---\n",
      "DB Code Enhanced prompt\n",
      " <prompt>\n",
      "  <persona>\n",
      "    You are an expert Python backend developer with deep, production-level experience in FastAPI and SQLAlchemy. Your answers are concise, precise, and adhere to best practices.\n",
      "  </persona>\n",
      "\n",
      "  <context>\n",
      "    You are connecting a FastAPI application to a SQLite database named “onboarding.db”.  \n",
      "    The database schema is:\n",
      "\n",
      "    ```sql\n",
      "    CREATE TABLE users (\n",
      "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        name TEXT NOT NULL,\n",
      "        email TEXT NOT NULL UNIQUE,\n",
      "        role TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager'))\n",
      "    );\n",
      "\n",
      "    CREATE TABLE onboarding_tasks (\n",
      "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "        title TEXT NOT NULL,\n",
      "        description TEXT NOT NULL,\n",
      "        due_date DATE NOT NULL,\n",
      "        status TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "        user_id INTEGER NOT NULL,\n",
      "        FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE\n",
      "    );\n",
      "    ```\n",
      "  </context>\n",
      "\n",
      "  <instructions>\n",
      "    1. Think step-by-step (internally) to design idiomatic SQLAlchemy model classes for the two tables.  \n",
      "    2. Write well-commented Python code that:\n",
      "       • Defines the declarative base.  \n",
      "       • Maps the `users` and `onboarding_tasks` tables to SQLAlchemy ORM models.  \n",
      "    3. Write a second, separate, well-commented code block that shows:\n",
      "       • Creation of the SQLAlchemy `engine` for the SQLite file `onboarding.db`.  \n",
      "       • Definition of `SessionLocal` using `sessionmaker`.  \n",
      "       • A FastAPI-compatible `get_db` dependency that yields a session and properly closes it.  \n",
      "    4. Ensure imports are included.  \n",
      "    5. Do NOT output any explanatory text—only the raw Python code inside two separate triple-back-tick blocks.\n",
      "  </instructions>\n",
      "\n",
      "  <output_format>\n",
      "    ```python\n",
      "    # Code block 1: SQLAlchemy models\n",
      "    ...\n",
      "    ```\n",
      "\n",
      "    ```python\n",
      "    # Code block 2: Database session management\n",
      "    ...\n",
      "    ```\n",
      "  </output_format>\n",
      "</prompt>\n",
      "\n",
      "--- Generated Database Code (cleaned) ---\n",
      "# Code block 1: SQLAlchemy models\n",
      "from sqlalchemy import Column, Integer, String, Date, ForeignKey, CheckConstraint\n",
      "from sqlalchemy.ext.declarative import declarative_base\n",
      "from sqlalchemy.orm import relationship\n",
      "\n",
      "# Create declarative base\n",
      "Base = declarative_base()\n",
      "\n",
      "\n",
      "class User(Base):\n",
      "    \"\"\"SQLAlchemy model for the users table.\"\"\"\n",
      "    __tablename__ = \"users\"\n",
      "    \n",
      "    # Primary key\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    \n",
      "    # User attributes\n",
      "    name = Column(String, nullable=False)\n",
      "    email = Column(String, nullable=False, unique=True, index=True)\n",
      "    role = Column(String, nullable=False)\n",
      "    \n",
      "    # Relationship to onboarding tasks\n",
      "    onboarding_tasks = relationship(\"OnboardingTask\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n",
      "    \n",
      "    # Table-level constraints\n",
      "    __table_args__ = (\n",
      "        CheckConstraint(\"role IN ('New Hire', 'Manager')\", name=\"check_role\"),\n",
      "    )\n",
      "\n",
      "\n",
      "class OnboardingTask(Base):\n",
      "    \"\"\"SQLAlchemy model for the onboarding_tasks table.\"\"\"\n",
      "    __tablename__ = \"onboarding_tasks\"\n",
      "    \n",
      "    # Primary key\n",
      "    id = Column(Integer, primary_key=True, index=True)\n",
      "    \n",
      "    # Task attributes\n",
      "    title = Column(String, nullable=False)\n",
      "    description = Column(String, nullable=False)\n",
      "    due_date = Column(Date, nullable=False)\n",
      "    status = Column(String, nullable=False)\n",
      "    \n",
      "    # Foreign key to users table\n",
      "    user_id = Column(Integer, ForeignKey(\"users.id\", ondelete=\"CASCADE\"), nullable=False)\n",
      "    \n",
      "    # Relationship to user\n",
      "    user = relationship(\"User\", back_populates=\"onboarding_tasks\")\n",
      "    \n",
      "    # Table-level constraints\n",
      "    __table_args__ = (\n",
      "        CheckConstraint(\"status IN ('Pending', 'Completed')\", name=\"check_status\"),\n",
      "    )\n",
      "Saved DB models and session code to app/db_models.py\n"
     ]
    }
   ],
   "source": [
    "db_code_prompt = f\"\"\"\n",
    "You are a Python expert specializing in FastAPI and SQLAlchemy.\n",
    "\n",
    "Based on the provided SQL schema, generate the necessary Python code to connect a FastAPI application to a SQLite database named 'onboarding.db'.\n",
    "\n",
    "**SQL Schema Context:**\n",
    "```sql\n",
    "{sql_schema}\n",
    "```\n",
    "\n",
    "Please provide two separate, well-commented code blocks:\n",
    "\n",
    "1.  **SQLAlchemy Models:** Create the Python classes that map to the `users` and `onboarding_tasks` tables.\n",
    "2.  **Database Session Management:** Provide the standard boilerplate code for creating the SQLAlchemy engine, the `SessionLocal` class, and the `get_db` dependency for FastAPI.\n",
    "\n",
    "Only output the raw Python code.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQLAlchemy Models and Session Code ---\")\n",
    "if sql_schema:\n",
    "    # Enhance the DB prompt to ensure precise, well-structured output from the model\n",
    "    enhanced_db_code_prompt = prompt_enhancer(db_code_prompt)\n",
    "    print(\"DB Code Enhanced prompt\\n\", enhanced_db_code_prompt)\n",
    "    generated_db_code = get_completion(enhanced_db_code_prompt, db_client, db_model_name, db_api_provider)\n",
    "    # Clean and save the generated DB code to an artifact so the integration step can use it.\n",
    "    cleaned_db_code = clean_llm_output(generated_db_code, language='python')\n",
    "    print(\"\\n--- Generated Database Code (cleaned) ---\")\n",
    "    print(cleaned_db_code)\n",
    "    save_artifact(cleaned_db_code, \"app/db_models.py\", overwrite=True)\n",
    "    print(\"Saved DB models and session code to app/db_models.py\")\n",
    "else:\n",
    "    print(\"Skipping DB code generation because schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Integrating Live Database Logic\n",
    "\n",
    "**Explanation:**\n",
    "This final code represents the crucial role of the developer in an AI-assisted workflow. The AI provided the components (Pydantic models, SQLAlchemy models, endpoint structure), but the developer is responsible for the final integration, ensuring all the pieces work together seamlessly. This involves combining the generated code blocks and replacing the in-memory list operations with live SQLAlchemy database calls (`db.add`, `db.query`, `db.commit`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 21:27:58,303 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Integrating generated artifacts into final app/main.py ---\n",
      "--- Final integrated app (preview) ---\n",
      "from typing import List\n",
      "from fastapi import Depends, FastAPI, HTTPException\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import BaseModel\n",
      "from app.db_models import SessionLocal, engine, Base, User as DBUser\n",
      "\n",
      "Base.metadata.create_all(bind=engine)\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class UserBase(BaseModel):\n",
      "    name: str\n",
      "    email: str\n",
      "\n",
      "class UserCreate(UserBase):\n",
      "    pass\n",
      "\n",
      "class User(UserBase):\n",
      "    id: int\n",
      "\n",
      "    class Config:\n",
      "        orm_mode = True\n",
      "\n",
      "def get_db():\n",
      "    db = SessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "@app.post(\"/users/\", response_model=User)\n",
      "def create_user(user: UserCreate, db: Session = Depends(get_db)):\n",
      "    db_user = DBUser(name=user.name, email=user.email)\n",
      "    db.add(db_user)\n",
      "    db.commit()\n",
      "    db.refresh(db_user)\n",
      "    return db_user\n",
      "\n",
      "@app.get(\"/users/\", response_model=List[User])\n",
      "def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):\n",
      "    users = db.query(DBUser).offset(skip).limit(limit).all()\n",
      "    return users\n",
      "\n",
      "@app.get(\"/users/{user_id}\", response_model=User)\n",
      "def read_user(user_id: int, db: Session = Depends(get_db)):\n",
      "    db_user = db.query(DBUser).filter(DBUser.id == user_id).first()\n",
      "    if db_user is None:\n",
      "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
      "    return db_user\n",
      "Saved integrated app to app/main.py\n",
      "--- Final Integrated API Code for app/main.py ---\n",
      "from typing import List\n",
      "from fastapi import Depends, FastAPI, HTTPException\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import BaseModel\n",
      "from app.db_models import SessionLocal, engine, Base, User as DBUser\n",
      "\n",
      "Base.metadata.create_all(bind=engine)\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "class UserBase(BaseModel):\n",
      "    name: str\n",
      "    email: str\n",
      "\n",
      "class UserCreate(UserBase):\n",
      "    pass\n",
      "\n",
      "class User(UserBase):\n",
      "    id: int\n",
      "\n",
      "    class Config:\n",
      "        orm_mode = True\n",
      "\n",
      "def get_db():\n",
      "    db = SessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "@app.post(\"/users/\", response_model=User)\n",
      "def create_user(user: UserCreate, db: Session = Depends(get_db)):\n",
      "    db_user = DBUser(name=user.name, email=user.email)\n",
      "    db.add(db_user)\n",
      "    db.commit()\n",
      "    db.refresh(db_user)\n",
      "    return db_user\n",
      "\n",
      "@app.get(\"/users/\", response_model=List[User])\n",
      "def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):\n",
      "    users = db.query(DBUser).offset(skip).limit(limit).all()\n",
      "    return users\n",
      "\n",
      "@app.get(\"/users/{user_id}\", response_model=User)\n",
      "def read_user(user_id: int, db: Session = Depends(get_db)):\n",
      "    db_user = db.query(DBUser).filter(DBUser.id == user_id).first()\n",
      "    if db_user is None:\n",
      "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
      "    return db_user\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/agaleana/repos/AG-AISOFTDEV/artifacts/app/main.py')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integration step: combine generated artifacts into a minimal live `app/main.py`\n",
    "print(\"--- Integrating generated artifacts into final app/main.py ---\")\n",
    "\n",
    "# Load generated artifacts from the artifacts folder (artifacts/app/*)\n",
    "in_memory_code = load_artifact(\"app/main_in_memory.py\")\n",
    "db_models_code = load_artifact(\"app/db_models.py\")\n",
    "\n",
    "integration_prompt = (\n",
    "    \"You are a pragmatic Python developer. \"\n",
    "    \"Produce a minimal `app/main.py` that imports SessionLocal, engine, and User from `app.db_models` \"\n",
    "    \"and implements POST /users/ (create), GET /users/ (list), GET /users/{user_id} (retrieve). \"\n",
    "    \"Use explicit SQLAlchemy calls (db.add, db.query, db.commit, db.refresh). \"\n",
    "    \"Do NOT inline large artifact sources; they are available at artifacts/app/main_in_memory.py and artifacts/app/db_models.py. \"\n",
    "    \"Use './artifacts/onboarding.db' as the SQLite file and ensure response models set orm_mode=True. \"\n",
    "    \"Return only the raw Python source for app/main.py.\"\n",
    ")\n",
    "\n",
    "# Combine prompt and metadata, then enhance\n",
    "prompt = prompt_enhancer(integration_prompt)\n",
    "\n",
    "# Request integration code from the integration model\n",
    "integration_output = get_completion(\n",
    "    prompt,\n",
    "    integration_client,\n",
    "    integration_model_name,\n",
    "    integration_api_provider,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Clean and persist the result\n",
    "cleaned_integration = clean_llm_output(integration_output, language=\"python\")\n",
    "print(\"--- Final integrated app (preview) ---\")\n",
    "print(cleaned_integration)\n",
    "save_artifact(cleaned_integration, \"app/main.py\", overwrite=True)\n",
    "print(\"Saved integrated app to app/main.py\")\n",
    "\n",
    "# Expose final_api_code for downstream cells that may reference it\n",
    "final_api_code = cleaned_integration\n",
    "print(\"--- Final Integrated API Code for app/main.py ---\")\n",
    "print(final_api_code)\n",
    "save_artifact(final_api_code, \"app/main.py\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Congratulations! You have successfully generated and assembled a complete, database-connected backend API. You used an LLM to generate the boilerplate for both the API endpoints and the database models, and then performed the crucial engineering task of integrating them. You now have a working `main.py` file in your `app` directory that can create, read, update, and delete data in a live database. In the next lab, we will write a comprehensive test suite for this API.\n",
    "\n",
    "> **Key Takeaway:** AI excels at generating boilerplate code (like models and endpoint structures), but the developer's critical role is in the final integration and wiring of these components into a coherent, working system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
